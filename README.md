# mashup-recommender

# Mashup Recommender ‚Äì ETL (Fase Extract)

## Estructura b√°sica

- `data/input/`  
  Coloca aqu√≠ los ficheros originales descargados de Kaggle:
  - `spotify_tracks.csv`
  - `apple_music.csv`

- `data/raw/`  
  Aqu√≠ se generar√°n los ficheros Parquet "raw":
  - `spotify_tracks.parquet`
  - `apple_music.parquet`

## Instalaci√≥n

```bash
python -m venv .venv
source .venv/bin/activate   # en Windows: .venv\Scripts\activate
pip install -r requirements.txt
python -m src.main_extract
python -m src.main_transform
python -m src.main_load
```

---

## üîü Qu√© estamos haciendo exactamente (en t√©rminos de ETL)

Solo para dejarlo claro conceptualmente:

- **E (Extract)**:  
  - Tomamos los **datasets originales** (Spotify y Apple Music) tal y como los hemos descargado.
  - Los leemos con pandas de forma robusta, comprobando que existen, n√∫mero de filas, etc.
  - Hacemos una **validaci√≥n ligera**: columnas importantes, nulos b√°sicos.

- **Resultado de esta fase**:  
  - Tenemos una ‚Äú**zona RAW**‚Äù (`data/raw/`) en un formato optimizado (Parquet), sobre el que ser√° m√°s c√≥modo y eficiente trabajar en las siguientes fases (**Transform** & **Load al Data Warehouse**).

A partir de aqu√≠, el siguiente paso natural ser√° la **T (Transform)**:  
- Homogeneizar columnas (nombres, tipos).  
- Empezar a definir el modelo com√∫n (track, artista, tonalidad, etc.).  
- Preparar las tablas para el futuro Data Warehouse.

---

Si quieres, en el siguiente mensaje podemos:

- Dise√±ar y codificar la **fase Transform** sobre estos Parquet (limpieza + normalizaci√≥n de tonos/BPM),  
o  
- A√±adir ya el **extract de alguna API** (por ejemplo, Spotify Web API para popularidad actual) y dejar otro m√≥dulo `extract_spotify_api.py`.
::contentReference[oaicite:0]{index=0}
