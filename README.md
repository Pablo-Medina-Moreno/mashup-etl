# ğŸµ ETL Musical

## ğŸ“Œ DescripciÃ³n del proyecto

Este repositorio implementa un **pipeline ETL completo** en Python que integra tres datasets musicales:

1. **Spotify Tracks (Kaggle)**
2. **Spotifyâ€“YouTube Dataset**
3. **Spotify Global Music (track_data_final.csv)**

El objetivo es construir un **modelo relacional normalizado en PostgreSQL**, consolidando informaciÃ³n sobre:

- Canciones (tracks)  
- Ãlbumes  
- Artistas  
- GÃ©neros de artistas  
- Relaciones Trackâ€“Artista (N:N)

El proyecto sigue la arquitectura clÃ¡sica:

â¡ï¸ **EXTRACT â†’ TRANSFORM â†’ INTEGRATE â†’ LOAD**

---

# ğŸ“ Estructura del proyecto
```
mashup-recommender/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ input/ â† CSV originales
â”‚ â”œâ”€â”€ raw/ â† Se genera en EXTRACT
â”‚ â””â”€â”€ processed/ â† Se genera en TRANSFORM
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ extract_spotify.py
â”‚ â”œâ”€â”€ extract_spotify_youtube.py
â”‚ â”œâ”€â”€ extract_track_data_final.py
â”‚ â”œâ”€â”€ transform_spotify.py
â”‚ â”œâ”€â”€ transform_spotify_youtube.py
â”‚ â”œâ”€â”€ transform_track_data_final.py
â”‚ â”œâ”€â”€ transform_integrated.py
â”‚ â”œâ”€â”€ load_schema.py
â”‚ â”œâ”€â”€ main_extract.py
â”‚ â”œâ”€â”€ main_transform.py
â”‚ â”œâ”€â”€ main_load.py
â”‚ â”œâ”€â”€ utils_io.py
â”‚ â””â”€â”€ utils_db.py
â”‚
â”œâ”€â”€ config.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸ”§ InstalaciÃ³n y ejecuciÃ³n del pipeline completo

## 1. Descomprimir el proyecto

Descargar el `.zip` y extraerlo. La estructura debe quedar asÃ­:

```
mashup-recommender/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ input/ â† CSV originales
â”‚
â”œâ”€â”€ src/
â”œâ”€â”€ config.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
--

## 2. Crear entorno virtual e instalar dependencias

### En Windows

```bash
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
```

### En Linux / MacOS

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```
# 3. Ejecutar el pipeline ETL completo

Ejecuta los siguientes comandos **en este orden** desde la raÃ­z del proyecto (donde estÃ¡ `src/` y `config.py`):

---

## ğŸŸ¦ 1. EXTRACT  
Lee los CSV de `data/input/`, normaliza columnas y genera los JSON RAW en `data/raw/`.

```bash
python -m src.main_extract
```
Salida generada:

```
data/raw/
   spotify_tracks_raw.json
   spotify_youtube_raw.json
   track_data_final_raw.json
```
## ğŸŸ© 2. TRANSFORM

Procesa y limpia cada dataset, los convierte en formato anidado (track, album, artists) y luego ejecuta la integraciÃ³n final.

```bash
python -m src.main_transform
```
Salida generada:
```
data/processed/
   spotify_tracks_clean.json
   spotify_youtube_clean.json
   track_data_final_clean.json
   songs_integrated.json   â† archivo maestro final
```
## ğŸŸ§ 3. LOAD

Carga el dataset integrado en PostgreSQL.

```bash
python -m src.main_load
```
